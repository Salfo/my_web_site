---
title: 'Topic Modeling: An Application'
author: Salfo Bikienga
date: '2017-11-11'
categories:
  - R
  - Text Analytics
  - Topic Modeling
  - LDA
tags:
  - R
  - LDA
  - Text Analytics
  - Topic Modeling
slug: topic-modeling-an-application
bibliography: reference2.bib
---



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>My work involves the use and the development of topic modeling algorithms. A surprising challenge I have had is communicating the output of topic modeling algorithms to people not familiar with text analytics. Here is my 10 cents explanation of the LDA output to my econ friends.</p>
<p>The use of text data for <a href="http://review.chicagobooth.edu/magazine/spring-2015/why-words-are-the-new-numbers" target="_blank">economic analysis</a> is gaining attractions. One popular analytical tool is Latent Dirichlet Allocation (LDA), also called topic modeling <span class="citation" data-cites="Blei2003">(Blei, Ng, and Jordan 2003)</span>. Succinctly put, topic modeling consists of collapsing a matrix (i.e a spreadsheet) of words counts into a reduced matrix of topics’ proportions within documents. For instance, assume we have a collection of 500 documents, each containing 2000 unique words; this collection of documents (called corpus) can be represented as a dataset of 500 observations and 2000 variables (each word being a variable). Each cell in the matrix represents the count of a word in a document. The matrix is just a regular spreadsheet of data. Clearly, it is almost impossible to draw any insight from that many variables. LDA allows us to collapse the high dimensional dataset into a lower dimension, say a dimension of 10. With 10 variables, there is a hope that some insight can be drawn from the data. Following is a demonstration of LDA.</p>
</section>
<section id="example-data" class="level1">
<h1>Example Data</h1>
<p>Let’s consider a dataset of U.S. governors’ State of the State Addresses (SoSA). In most states, the governor gives a speech, generally in January, in which he/she lays out his/her priorities for the next fiscal year. Part of the goal of the speech is to explain (or justify) the proposed budget, and hopefully convince the state stakeholders to support the proposed budget. A budget proposal usually involves a reallocation of the state resources, which implies cuts and increases in different lines of the state budget. I collected 596 speeches from governors of the 50 states, spanning from 2001 to 2013.</p>
<p>It is customary in text analytics to delete words that we believe are not “discriminative”. For instance link words such as “the”, “and”, “she”, etc. will not distinguish a Democrat from a Republican. We call this process, pre-processing the data, that is, cleaning the data by removing elements in the texts that we believe are not useful for our analysis.</p>
<p>After pre-processing the data, I am left with a dataset of 596 observations and 1034 words (or variables). You can take a look at the pre-processed data <a href="http://rpubs.com/sbikienga/334137" target="_blank">here</a>, or you can download it <a href="https://github.com/Salfo/States-Addresses/raw/master/data/SoSA_data_df.RData" target="_blank">here</a>. Stemming, that is stripping the words to their roots, is often done to avoid counting related words separately. For example, education, educational, educate are stemmed and become educ.</p>
</section>
<section id="example-application-of-lda" class="level1">
<h1>Example application of LDA</h1>
<p>The goal when using LDA is primarily to reduce the dimension of a counts dataset. The hope is that the reduced dimension preserves the essential information contained in the original dataset. Interestingly, the reduced dimension is often more appropriate for statistical analysis, as it “solves” the overfitting problem associated with high dimensional data. Generally, the overfitting problem arises in situations where <span class="math inline">\(n\)</span>, the number of observations, is not big enough to provide reliable estimates of the <span class="math inline">\(p\)</span> variables’ parameters.</p>
<p>There are several packages in R to implement the LDA model (<code>lda</code>, <code>mallet</code>, and <code>topicmodels</code>). Here I will use the <code>topicmodels</code> package as an example.</p>
<pre class="r"><code># install.packages(&quot;topicmodels&quot;) # You should run this code once if you don&#39;t have topicmodels installed
library(topicmodels) # Load the topicmodels package
url &lt;- url(&quot;https://github.com/Salfo/States-Addresses/raw/master/data/SoSA_data_df.RData&quot;)
load(url) # Load the data from the url provided
SoSA_topics &lt;- LDA(SoSA_data_df, # The matrix of words counts
                   k = 2, # The number of topics to construct
                   method = &quot;Gibbs&quot;, # Estimation method
                   control = list(iter = 3000, # Number of iterations
                                  burnin = 1000, # Thow out the first 1000 estimates
                                  seed = 123)) # To get a reproducible results</code></pre>
<p>Note that LDA is a matrix factorization algorithm, and a matrix factorization consists of decomposing a matrix into the product of two or more matrices. Intuitively, we can write: <span class="math display">\[W_{D,V} \simeq \theta_{D,V}\phi_{K,V}\]</span></p>
<section id="the-reduced-dimension-theta-matrix" class="level2">
<h2>The reduced dimension, <span class="math inline">\(\theta\)</span> matrix</h2>
<p>In this example, <span class="math inline">\(D=596\)</span>, <span class="math inline">\(V=1034\)</span>, <span class="math inline">\(K=2\)</span>. <span class="math inline">\(\theta\)</span> contains the essential information needed to understand the variation between observations, concerning the speeches. For instance, <span class="math inline">\(\theta\)</span> can be used to study how Democrats differ from Republicans regarding the relative importance of themes they cover in their speeches. <span class="math inline">\(\theta\)</span> can be seen as a regular spreadsheet of data, as shown below. For an extended exposition of LDA, see <a href="http://www.salfobikienga.rbind.io/post/introduction-to-lda/" target="_blank">this</a>.</p>
<pre class="r"><code>theta_matrix &lt;- posterior(SoSA_topics)$topics # Extract the theta matrix
theta_matrix &lt;- round(as.data.frame(theta_matrix), digits = 3)
names(theta_matrix) &lt;- paste(&quot;Topic.&quot;, 1:2, sep = &quot;&quot;) # Name the columns
head(theta_matrix, n = 10) # Print out the first 10 observations</code></pre>
<pre><code>##                       Topic.1 Topic.2
## Alabama_2001_D_1.txt    0.274   0.726
## Alabama_2002_D_2.txt    0.377   0.623
## Alabama_2003_R_3.txt    0.767   0.233
## Alabama_2004_R_4.txt    0.613   0.387
## Alabama_2005_R_5.txt    0.484   0.516
## Alabama_2006_R_6.txt    0.513   0.487
## Alabama_2007_R_7.txt    0.424   0.576
## Alabama_2008_R_8.txt    0.481   0.519
## Alabama_2009_R_9.txt    0.516   0.484
## Alabama_2010_R_10.txt   0.583   0.417</code></pre>
</section>
<section id="how-do-we-know-which-themes-are-covered" class="level2">
<h2>How do we know which themes are covered?</h2>
<p>Well, here we imposed the number of themes by setting <span class="math inline">\(K=2\)</span>. To identify the themes, we use the matrix <span class="math inline">\(\phi\)</span>, which presents the relative importance of each word for each theme (or topic).</p>
<pre class="r"><code>phi_matrix &lt;- posterior(SoSA_topics)$terms # Extract the phi matrix
phi_matrix &lt;- round(phi_matrix, 3) # Round the numbers to 3 decimals
phi_matrix[, 1:20] # Print out the first 20 words</code></pre>
<pre><code>##    abil  abus academ acceler accept access accomplish accord account
## 1 0.001 0.001  0.000       0  0.001  0.000      0.000      0   0.002
## 2 0.000 0.001  0.001       0  0.000  0.003      0.001      0   0.001
##   achiev acknowledg across action activ actual addit address adequ
## 1  0.001      0.001  0.001  0.001 0.001  0.001 0.003   0.005     0
## 2  0.002      0.000  0.002  0.001 0.001  0.000 0.001   0.001     0
##   administr adopt
## 1     0.003 0.001
## 2     0.000 0.000</code></pre>
<p>It might be more helpful to transpose the <span class="math inline">\(\phi\)</span> so that by sorting each topic by decreasing order of the words relative weights we can identify the first few most important (in terms of weight) words for the given topic.</p>
<pre class="r"><code>T_phi_matrix &lt;- as.data.frame(t(phi_matrix))
names(T_phi_matrix) &lt;- paste(&quot;Topic.&quot;, 1:2)
T_phi_matrix[1:20, ] # Print out the first 20 words</code></pre>
<pre><code>##            Topic. 1 Topic. 2
## abil          0.001    0.000
## abus          0.001    0.001
## academ        0.000    0.001
## acceler       0.000    0.000
## accept        0.001    0.000
## access        0.000    0.003
## accomplish    0.000    0.001
## accord        0.000    0.000
## account       0.002    0.001
## achiev        0.001    0.002
## acknowledg    0.001    0.000
## across        0.001    0.002
## action        0.001    0.001
## activ         0.001    0.001
## actual        0.001    0.000
## addit         0.003    0.001
## address       0.005    0.001
## adequ         0.000    0.000
## administr     0.003    0.000
## adopt         0.001    0.000</code></pre>
<p>The <code>terms()</code> function of the <code>topicmodels</code> package returns a convenient <span class="math inline">\(\phi\)</span> matrix that replaces the words weights by the words themselves, after sorting each row of the <span class="math inline">\(\phi\)</span> matrix.</p>
<pre class="r"><code>terms_matrix &lt;- terms(SoSA_topics, 30) # Extract the first 30 most important words for each topic
terms_matrix[1:15, ] # Print out the first 15 words</code></pre>
<pre><code>##       Topic 1   Topic 2   
##  [1,] &quot;budget&quot;  &quot;school&quot;  
##  [2,] &quot;fund&quot;    &quot;work&quot;    
##  [3,] &quot;govern&quot;  &quot;educ&quot;    
##  [4,] &quot;peopl&quot;   &quot;help&quot;    
##  [5,] &quot;million&quot; &quot;children&quot;
##  [6,] &quot;work&quot;    &quot;make&quot;    
##  [7,] &quot;make&quot;    &quot;famili&quot;  
##  [8,] &quot;public&quot;  &quot;nation&quot;  
##  [9,] &quot;propos&quot;  &quot;busi&quot;    
## [10,] &quot;servic&quot;  &quot;creat&quot;   
## [11,] &quot;dollar&quot;  &quot;health&quot;  
## [12,] &quot;know&quot;    &quot;student&quot; 
## [13,] &quot;spend&quot;   &quot;invest&quot;  
## [14,] &quot;increas&quot; &quot;teacher&quot; 
## [15,] &quot;program&quot; &quot;care&quot;</code></pre>
<p>By exploring the most important words for each topic, it seems reasonable to infer that Topic.1 is about “money”, the budget; and Topic.2 is mostly about education.</p>
<p>In sum, <span class="math inline">\(\theta\)</span> provides the essential information needed to understand variations or differences between observations; <span class="math inline">\(\phi\)</span> is used to infer the meaning of each of the <span class="math inline">\(K\)</span> columns of <span class="math inline">\(\theta\)</span>.</p>
</section>
</section>
<section id="using-theta-for-statistical-analysis" class="level1">
<h1>Using <span class="math inline">\(\theta\)</span> for statistical analysis</h1>
<p>Of what uses can we make of <span class="math inline">\(\theta\)</span>? Quite a lot!</p>
<p><span class="math inline">\(\theta\)</span> alone, or combined with other control variables, <span class="math inline">\(X\)</span>, can be used for regular statistical analysis. <span class="math inline">\(\theta\)</span> has been used for economic analyses. <span class="citation" data-cites="Brown2016">(Brown, Crowley, and Elliott 2016)</span> applied LDA to assess whether the thematic content of financial statement disclosures is informative in predicting intentional misreporting. <span class="citation" data-cites="Hansen2016">(Hansen and McMahon 2016)</span> uses LDA in a Factor Augmented Vector Autoregressive modeling framework. I have a working paper exploring the relationship between US governors commitments to their economic agenda as stated in their public statements and the expansion of business establishments in their states <span class="citation" data-cites="Bikienga2017">(Bikienga 2017)</span>. For a survey of the use of LDA and other text analytics tools in economics, see <span class="citation" data-cites="Gentzkow2017">(Gentzkow, Kelly, and Taddy 2017)</span>.</p>
</section>
<section id="illustration-of-the-use-of-theta" class="level1">
<h1>Illustration of the use of <span class="math inline">\(\theta\)</span></h1>
<p>Is there any difference between Democrats and Republicans based on the themes covered in their speeches? To answer this question, we can compute the mean values of the topics by party line. Note that D, R, or I is appended to the rownames of the <span class="math inline">\(\theta\)</span> shown above. They stand for Democrat, Republican, or Independent.</p>
<p>Here, I am using the rownames to construct additional variables (<code>state</code>, <code>party</code>, and <code>year</code>)</p>
<pre class="r"><code>library(stringr)
state_vars &lt;- row.names(theta_matrix) %&gt;% 
  str_split(pattern = &quot;_&quot;) %&gt;% as.data.frame() %&gt;% t()
state_vars &lt;- state_vars[, -4]
state_vars &lt;- data.frame(state_vars)
names(state_vars) &lt;- c(&quot;state&quot;, &quot;year&quot;, &quot;party&quot;)
df &lt;- data.frame(theta_matrix, state_vars)
n_obs &lt;- sample(1:596, size = 10)
sample_obs &lt;- df[n_obs,]
sample_obs</code></pre>
<pre><code>##                              Topic.1 Topic.2         state year party
## Mississippi_2001_D_245.txt     0.444   0.556   Mississippi 2001     D
## NewJersey_2009_D_307.txt       0.477   0.523     NewJersey 2009     D
## Alabama_2002_D_2.txt           0.377   0.623       Alabama 2002     D
## Oregon_2001_D_528.txt          0.644   0.356        Oregon 2001     D
## Massachusetts_2005_R_216.txt   0.413   0.587 Massachusetts 2005     R
## Indiana_2002_D_142.txt         0.444   0.556       Indiana 2002     D
## Missouri_2006_R_261.txt        0.536   0.464      Missouri 2006     R
## Ohio_2002_R_345.txt            0.269   0.731          Ohio 2002     R
## NewMexico_2002_R_311.txt       0.690   0.310     NewMexico 2002     R
## Maryland_2003_R_204.txt        0.435   0.565      Maryland 2003     R</code></pre>
<p>Compute the topics’ means by party line.</p>
<pre class="r"><code>library(dplyr)
library(tidyr)
df_by_party &lt;- df %&gt;%
  group_by(party) %&gt;%
summarise(Topic.1 = mean(Topic.1), Topic.2 = mean(Topic.2)) %&gt;%
  gather(Topic, Topic_proportion, Topic.1:Topic.2) %&gt;%
  mutate(Topic_proportion = round(100*Topic_proportion, 0)) %&gt;%
  mutate(pos = c(rep(75, 3), rep(25, 3)))
df_by_party</code></pre>
<pre><code>## # A tibble: 6 x 4
##   party Topic   Topic_proportion   pos
##   &lt;fct&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;
## 1 D     Topic.1              46.   75.
## 2 I     Topic.1              62.   75.
## 3 R     Topic.1              51.   75.
## 4 D     Topic.2              54.   25.
## 5 I     Topic.2              38.   25.
## 6 R     Topic.2              49.   25.</code></pre>
<p>Democrats seem to talk more about education (Topic.2) than Republicans. On average, about 54% of their speeches refers to the education theme, against 49% for Republicans. Conversely, Republicans tend to talk more about budgetary issues than Democrats (51% for Republicans vs. 46% for Democrats).</p>
<p>Clearly, these differences are not huge, and we cannot put too much stock into it. The goal here is to illustrate how one may use the topics distributions, without going into the intricacies of statistical significance.</p>
<p>The above table can be visualized with the help of a stacked bar plot.</p>
<pre class="r"><code>library(ggplot2)
library(ggthemes)
library(extrafont)
#library(plyr)
#library(scales)
fill &lt;- c(&quot;#add8e6&quot;, &quot;#b87333&quot;)
p_party &lt;- ggplot() +
  geom_bar(aes(y = Topic_proportion, x = party, fill = Topic), 
           data = df_by_party, stat=&quot;identity&quot;) +
  geom_text(data=df_by_party, aes(x = party, y = pos, label = paste0(Topic_proportion,&quot;%&quot;)),
            colour=&quot;black&quot;, family=&quot;Tahoma&quot;, size=4) +
  theme(legend.position=&quot;bottom&quot;, legend.direction=&quot;horizontal&quot;,
        legend.title = element_blank()) +
  labs(x=&quot;Political Party&quot;, y=&quot;Percentage&quot;) +
  ggtitle(&quot;Average Proportion of Topic Covered By Party (%)&quot;) +
  scale_fill_manual(values=fill) +
  theme(axis.line = element_line(size=1, colour = &quot;black&quot;),
        panel.grid.major = element_line(colour = &quot;#d3d3d3&quot;), panel.grid.minor = element_blank(),
        panel.border = element_blank(), panel.background = element_blank()) +
  theme(plot.title = element_text(size = 14, family = &quot;Tahoma&quot;, face = &quot;bold&quot;),
        text=element_text(family=&quot;Tahoma&quot;),
        axis.text.x=element_text(colour=&quot;black&quot;, size = 10),
        axis.text.y=element_text(colour=&quot;black&quot;, size = 10))
p_party</code></pre>
<p><img src="/post/2017-11-11-topic-modeling-an-application_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</section>
<section id="should-we-trust-the-results" class="level1">
<h1>Should we trust the results?</h1>
<p>Yes! We should. A mental block I faced when I started exploring topic modeling is trusting the results. If your program is like mine, latent variables models are not covered in your econometrics classes, even though they are widely used in the economics literature. In Macroeconomics, they are termed Factor Augmented Vector Autoregressive models. In Development Economics, they are used to construct indices <span class="citation" data-cites="Berenger2007">(Bérenger and Verdier-Chouchane 2007, <span class="citation" data-cites="Tabellini2010">@Tabellini2010</span>)</span>. Factor models approaches are also used as instruments <span class="citation" data-cites="Bai2010">(Bai and Ng 2010)</span>.</p>
<p>But, LDA is just another factor model algorithm. It is closely related to principal component analysis (PCA). In the future, I will present the idea of factor models, and why they are “reliable”.</p>
<p>#Conclusion</p>
<p>In sum, topic modeling in general and LDA in particular is a dimension reduction method. It consists of collapsing a matrix of words counts into a reduced matrix of topics distributions. This illustration provides a sense of its usefulness for statistical analysis.</p>
</section>
<section id="references" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-Bai2010">
<p>Bai, Jushan, and Serena Ng. 2010. “Instrumental Variable Estimation in a Data Rich Environment.” <em>Econometric Theory</em> 26 (6). Cambridge University Press: 1577–1606.</p>
</div>
<div id="ref-Berenger2007">
<p>Bérenger, Valérie, and Audrey Verdier-Chouchane. 2007. “Multidimensional Measures of Well-Being: Standard of Living and Quality of Life Across Countries.” <em>World Development</em> 35 (7). Elsevier: 1259–76.</p>
</div>
<div id="ref-Bikienga2017">
<p>Bikienga, Salfo. 2017. “The Governor as the Entrepreneur in Chief: An Exploratory Analysis.”</p>
</div>
<div id="ref-Blei2003">
<p>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” <em>J. Mach. Learn. Res.</em> 3 (March). JMLR.org: 993–1022. <a href="http://dl.acm.org/citation.cfm?id=944919.944937" class="uri">http://dl.acm.org/citation.cfm?id=944919.944937</a>.</p>
</div>
<div id="ref-Brown2016">
<p>Brown, Nerissa C, Richard M Crowley, and W Brooke Elliott. 2016. “What Are You Saying? Using Topic to Detect Financial Misreporting.”</p>
</div>
<div id="ref-Gentzkow2017">
<p>Gentzkow, Matthew, Bryan T Kelly, and Matt Taddy. 2017. “Text as Data.” National Bureau of Economic Research.</p>
</div>
<div id="ref-Hansen2016">
<p>Hansen, Stephen, and Michael McMahon. 2016. “Shocking Language: Understanding the Macroeconomic Effects of Central Bank Communication.” <em>Journal of International Economics</em> 99. Elsevier: S114–S133.</p>
</div>
<div id="ref-Tabellini2010">
<p>Tabellini, Guido. 2010. “Culture and Institutions: Economic Development in the Regions of Europe.” <em>Journal of the European Economic Association</em> 8 (4). Oxford University Press: 677–716.</p>
</div>
</div>
</section>
