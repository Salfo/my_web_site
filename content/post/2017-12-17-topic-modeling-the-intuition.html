---
title: 'Topic modeling: The Intuition'
author: Salfo Bikienga
date: '2017-11-17'
categories:
  - R
  - Text Analytics
  - Topic Modeling
tags:
  - R
  - Linear Regression
  - Matrix Factorization
  - OLS
slug: topic-modeling-the-intuition
---



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Whenever I give a talk on topic modeling to people not familiar with the subject, the usual question I receive is: “can you provide some intuition behind topic modeling?” Another variant of the same question is: “This is magic. How can the computer identify the topics in the documents?”. No! It is not magic. It is Math. I presented the math behind <a href="http://www.salfobikienga.rbind.io/post/introduction-to-lda/" target="_blank">Latent Dirichlet Allocation</a>, and an <a href="http://www.salfobikienga.rbind.io/post/topic-modeling-an-application/" target="_blank">example apllication</a> in previous posts. Here is my attempt at providing the intuition from the perspective of someone with basic understanding of simple linear regression, and a bit of matrix algebra.<br />
Topic modeling is a form of matrix factorization. Though modern topic modeling algorithms involve complex probability theory, the basic intuition can be developed through simple matrix factorization.<br />
Matrix factorization can be understood as a form of data dimension reduction method. In a world of “big data”, the usefulness of such method is immense. For instance, linear regression, the most used statistical tool in economics is only applicable when <span class="math inline">\(n\)</span>, the number of observations is at least as big as <span class="math inline">\(p\)</span>, the number of variables. When <span class="math inline">\(p\)</span> is too big, we resort to some dimension reduction methods such as choosing a few variables based on theory, using <a href="https://en.wikipedia.org/wiki/Stepwise_regression" target="_blank">stepwise</a>, or <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" target="_blank">LASSO</a> regression. With matrix factorization, we do not have to select variables. We can just “redifined” the variables in a lower dimensional space, that is, convert the <span class="math inline">\(p\)</span> dimensional data into <span class="math inline">\(k\)</span> dimensional data, where <span class="math inline">\(k\)</span> is significantly less than <span class="math inline">\(p\)</span> (<span class="math inline">\(k&lt;&lt;p\)</span>). The question is, how does that make sense? It is just matrix algebra, as you will see very soon.</p>
</section>
<section id="the-idea-of-dimension-reduction-from-matrix-factorization-perspective" class="level1">
<h1>The idea of dimension reduction from matrix factorization perspective</h1>
<p>1- Consider measures of <code>length</code>, <code>width</code>, and <code>depth</code>. These are three variables, i.e. three dimensional data. If size is enough information we care about, then <code>volume</code>, that is, <code>Volume = length x width x depth</code> is a good variable. Thus, we can collapse the three variables (<code>length</code>, <code>width</code>, <code>depth</code>) into a single variable, <code>volume</code> and preserve the essential information needed.</p>
<p>2- Consider measures of <code>height</code>, <code>weigth</code>, and <code>waist</code>. These are three variables, i.e. three dimensional data. If size provides enough information for what we need, then some form of linear combination of the three variables (<span class="math inline">\(size = b_1\times height + b_2 \times weight + b_3 \times waist\)</span>) will do. Thus, we collapse the three dimensional data into a one dimensional data.</p>
<p>3- Consider a dataset of words counts in several documents. Let’s consider the following words: <code>college</code>, <code>drugs</code>, <code>education</code>, <code>graduation</code>, <code>health</code>, <code>medicaid</code>. This is a six dimensional data. If what we care about are the concepts of education and health care, then some form of linear combination of these words counts will do. Our task consists of finding the appropriate weights so that a document having higher counts of education related words than other words gets a high value for the education concept, and low value for the health concept. And a document having higher counts of health related words than other words gets a high value for the health concept, and a low value for the education concept. Thus, we reduce the six dimensional data into two dimensional data, while preserving the essential information we care about.</p>
</section>
<section id="the-idea-of-matrix-factorization" class="level1">
<h1>The idea of matrix factorization</h1>
<p>The idea of matrix factorization stems from the fact that any matrix can be decomposed into the product of two or more matrices. Let <span class="math inline">\(W_{n,p}\)</span> be a matrix of dataset with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns. We can write the same matrix as the product of two matrices, such as: <span class="math display">\[\begin{equation}
W_{n,p} \simeq Z_{n,k}B_{k,p}
\label{eq:fac1}
\end{equation}\]</span> It turns out that <span class="math inline">\(Z\)</span> preserves the essential information needed to understand variations between the <span class="math inline">\(n\)</span> observations in <span class="math inline">\(W\)</span>.</p>
<section id="illustrative-example" class="level2">
<h2>Illustrative example</h2>
<p>Let <span class="math inline">\(W_{n,p}\)</span> be a spreadsheet of words counts in <span class="math inline">\(n\)</span> documents. <span class="math inline">\(p\)</span> is the number of unique words, and can be seen as variables. Here, <span class="math inline">\(n=6\)</span>, <span class="math inline">\(p = 5\)</span>, <span class="math inline">\(k = 2\)</span>. Let <code>college</code>, <code>education</code>, <code>family</code>, <code>health</code>, and <code>medicaid</code> be respectively the variables names of the <span class="math inline">\(W\)</span> matrix.</p>
<p><span class="math display">\[ 
\underbrace{\begin{bmatrix}
4&amp;6&amp;0&amp;2&amp;2  \\ 
0&amp;0&amp;4&amp;8&amp;12  \\ 
6&amp;9&amp;1&amp;5&amp;6 \\    
2&amp;3&amp;3&amp;7&amp;10 \\
0&amp;0&amp;3&amp;6&amp;9 \\
4&amp;6&amp;1&amp;4&amp;5 \\
    \end{bmatrix}
        }_{\mathbf{W_{6,5}}}
=
\underbrace{\begin{bmatrix} 
2&amp;0  \\
0&amp;4  \\
3&amp;1 \\
1&amp;3 \\
0&amp;3 \\
2&amp;1 \\
    \end{bmatrix}
        }_{\mathbf{Z_{6,2}}} 
\underbrace{\begin{bmatrix} 
2&amp;3&amp;0&amp;1&amp;1 \\
0&amp;0&amp;1&amp;2&amp;3 \\             
    \end{bmatrix}
        }_{\mathbf{B_{2,5}}} 
\]</span></p>
<p>It is easy to check that the product holds, that is, <span class="math inline">\(Z_{6,2}B_{2,5} = W_{6,5}\)</span>. <span class="math inline">\(Z\)</span> contains most of the information about the observations contained in <span class="math inline">\(W\)</span>. With <span class="math inline">\(Z\)</span>, we can explore, or study the variation between the observations, easily. <span class="math inline">\(Z\)</span> is a two dimensional representation of <span class="math inline">\(W\)</span>, and a simple scatterplot can be used to explore the data, as shown in the plot below.</p>
<pre class="r"><code>Z &lt;- matrix(c(2, 0,
             0, 4,
             3, 1,
             1, 3,
             0, 3,
             2, 1), byrow = TRUE, nrow = 6)
Z &lt;- data.frame(z1 = Z[,1], z2 = Z[, 2])
plot(x = Z$z1, y = Z$z2, cex = 3)
text(x = Z$z1, y = Z$z2, labels= 1:6, cex= 1)</code></pre>
<p><img src="/post/2017-12-17-topic-modeling-the-intuition_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>From the plot, we can deduce that observations (or documents) 2, 4 and 5 are close to each other; observations 1, 6 and 3 are also close to each other. The point here is that with a reduced dimension, it is easier to draw some insight from the data. Hence, the benefit of matrix factorization for data analysis.</p>
<p>For a <strong>predictive modeling</strong> exercise, we replace the <span class="math inline">\(W\)</span> matrix with the <span class="math inline">\(Z\)</span> matrix, and the usual tools (linear regression, logistic regression, regression tree, etc.) can be used. We do not have to understand the meaning of the new variables <span class="math inline">\(Z\)</span>. We only care about their ability to predict.<br />
However, for <strong>exploratory</strong> and <strong>inferential</strong> data analysis, we want to understand the meaning of the new variables <span class="math inline">\(Z\)</span>. To tell a story, we have to know the meaning of the variables. We infer the meaning of the new variables by inspecting the <span class="math inline">\(B\)</span> matrix. I will explain why that is the case shortly. For now, note that the number of columns of <span class="math inline">\(B\)</span> is the number of columns of the <span class="math inline">\(W\)</span> matrix; and the number of its rows is the number of columns of the <span class="math inline">\(Z\)</span> matrix. Each row of <span class="math inline">\(B\)</span> is used to interpret the meaning of each column of <span class="math inline">\(Z\)</span>. Row <span class="math inline">\(i\)</span> of <span class="math inline">\(B\)</span> is used to infer the meaning of column <span class="math inline">\(i\)</span> of <span class="math inline">\(Z\)</span>. For instance, referring to our illustrative example above, row 1 of <span class="math inline">\(B\)</span> has its biggest values at its first and second column, that is variables 1 and 2 (<code>college</code> and <code>education</code>) of the <span class="math inline">\(W\)</span> matrix are dominant in the identification of the meaning of the first <span class="math inline">\(Z\)</span> variable. Likewise, row 2 of <span class="math inline">\(B\)</span> has its biggest values at its two last columns; the variables 4 and 5 (<code>health</code> and <code>medicaid</code>) of the <span class="math inline">\(W\)</span> matrix are dominant in the identification of the meaning of the second <span class="math inline">\(Z\)</span> variable. Thus, the columns of <span class="math inline">\(Z\)</span> represent, respectively, measures of education and health concepts.</p>
</section>
<section id="finding-z-and-b" class="level2">
<h2>Finding <span class="math inline">\(Z\)</span> and <span class="math inline">\(B\)</span></h2>
<p>There are several matrix factorization algorithms. Factor Analysis (FA), Principal Component Analysis (PCA), Non Negative Matrix Factorization (NMF), Probabilistic Semantic Analysis (PLSA) and its variants, etc. Since our goal for this introduction is to present the basic idea, let’s present an algorithm that is closer to something we are all familiar with: Ordinary Least Squares (OLS).</p>
<section id="multivariate-ols" class="level3">
<h3>Multivariate OLS</h3>
<p>From introductory statistics, we know that for: <span class="math display">\[y_{n,1} = X_{n,p}\beta_{p,1} + \epsilon_{n,1}\]</span> the least squares solution for <span class="math inline">\(\beta_{p,1}\)</span> is: <span class="math display">\[\hat\beta_{p,1} = (X^tX)^{-1}X^ty\]</span> We are assuming that <span class="math inline">\((X^tX)^{-1}\)</span> exists. <span class="math inline">\(t\)</span> stands for transpose, and <span class="math inline">\(-1\)</span> stands for inverse.</p>
<p>In case you do not remember this formula, recall that: <span class="math display">\[y_{n,1} = X_{n,p}\beta_{p,1} + \epsilon_{n,1}
\Leftrightarrow 
X^tY = (X^tX)\beta + X^t\epsilon\]</span> Under the assumptions of no correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(E(\epsilon) = 0\)</span>, we can set <span class="math inline">\(X^t\epsilon=0\)</span>. So we have: <span class="math display">\[X^tY = (X^tX)\beta \\
\Leftrightarrow \\
(X^tX)^{-1}X^tY = (X^tX)^{-1}(X^tX)\beta \\
\Rightarrow \\
\hat\beta = (X^tX)^{-1}X^tY
\]</span></p>
<p>For a more than a single left hand side variable <span class="math inline">\(y_{n,1}\)</span>, the same formula applies; and we have: <span class="math display">\[\hat B = (X^tX)^{-1}X^tY\]</span> where <span class="math inline">\(B\)</span> is a <span class="math inline">\(p\times q\)</span> matrix, and <span class="math inline">\(Y\)</span> is a <span class="math inline">\(n \times q\)</span> matrix.</p>
</section>
<section id="multivariate-ols-and-matrix-factorization" class="level3">
<h3>Multivariate OLS and matrix factorization</h3>
<p>What does multivariate regression have to do with matrix factorization? Note that, ignoring the <span class="math inline">\(\epsilon\)</span>, we could have written: <span class="math display">\[\begin{equation}
Y_{n,q} \simeq X_{n,p}B_{p,q}
\label{eq:ols}
\end{equation}\]</span> This equation is very similar to the equation <span class="math inline">\(W_{n,p} \simeq Z_{n,k}B_{k,p}\)</span>, except <span class="math inline">\(X\)</span> is observed for the case of the multivariate OLS.</p>
<p>In multivariate OLS, we only estimate <span class="math inline">\(B\)</span>. For matrix factorization, we estimate <span class="math inline">\(Z\)</span> and <span class="math inline">\(B\)</span>.<br />
From <span class="math inline">\(W \simeq ZB\)</span>, we can solve for <span class="math display">\[\hat B = (Z^tZ)^{-1}Z^tW\]</span> or <span class="math display">\[\hat Z = WB^t(BB^t)^{-1}\]</span> The predicted values for <span class="math inline">\(W\)</span> is: <span class="math display">\[\hat W = \hat Z \hat B\]</span> To estimate <span class="math inline">\(B\)</span>, we need <span class="math inline">\(Z\)</span>, and to estimate <span class="math inline">\(Z\)</span> we need <span class="math inline">\(B\)</span>. We do not have either one. The trick is to guess some initial values for <span class="math inline">\(Z\)</span>, and use it to estimate <span class="math inline">\(B\)</span>, then use the estimated <span class="math inline">\(B\)</span> to estimate a new <span class="math inline">\(Z\)</span>. Use the new <span class="math inline">\(Z\)</span> to estimate a new <span class="math inline">\(B\)</span>. Continue the iteration untill some stopping criterion. Thus, we estimate <span class="math inline">\(Z\)</span> and <span class="math inline">\(B\)</span> iteratively (This estimation method is known as Alternating Least Squares). When do we stop the iteration?</p>
<p>Again, <span class="math inline">\(\hat W = \hat Z \hat B\)</span> is the predicted values for <span class="math inline">\(W\)</span>. We itterate until the distance between <span class="math inline">\(W\)</span> and its predicted value, <span class="math inline">\(\hat W\)</span>, is negligible. There are several distance measures, but let’s keep things simple by using the euclidean distance, or <span class="math inline">\(L_2\)</span> norm: <span class="math display">\[Q(\hat Z, \hat B) = ||W-\hat W (\hat Z, \hat B)||_2 = \sqrt{\sum_{i = 1}^n \sum_{j = 1}^p (w_{i,j} - \hat w_{i,j})^2}\]</span> Thus, we minimize <span class="math inline">\(Q\)</span>, the objective function. Following is an example implementation of a simple alternating least squares algorithm.</p>
<pre class="r"><code>W &lt;- matrix(c(4,    6,    0,    2,    2,
             0,    0,    4,    8,   12,
             6,    9,    1,    5,    6,
             2,    3,    3,    7,   10,
             0,    0,    3,    6,    9,
             2,    6,    1,    4,    5), byrow = TRUE, nrow = 6)

set.seed(3)
Z_init &lt;- abs(round(rnorm(n = 6*2, mean = 0, sd = 2),0))
Z_init &lt;- matrix(Z_init, nrow = 6)

Z &lt;- Z_init
dist_ww &lt;- 1e3
max_iter &lt;- 1000
iter &lt;- 0
while(iter &lt;= max_iter &amp;&amp; dist_ww &gt;= 1e-6) {
  iter &lt;- iter + 1
  ZZ_inv &lt;- solve(t(Z)%*%Z)
  B &lt;- ZZ_inv%*%t(Z)%*%W
  BB_inv &lt;- solve(B%*%t(B))
  Z &lt;- W%*%t(B)%*%BB_inv
  W_hat &lt;- Z%*%B
  dist_ww &lt;- sqrt(sum(W-W_hat)^2)
}
W &lt;- data.frame(W)
names(W) &lt;- c(&quot;college&quot;, &quot;education&quot;, &quot;family&quot;, &quot;health&quot;, &quot;medicaid&quot;)
Z &lt;- data.frame(round(Z, 2))
row.names(Z) &lt;- paste0(&quot;document.&quot;, 1:6)
names(Z) &lt;- c(&quot;Topic.1&quot;, &quot;Topic.2&quot;)
B &lt;- data.frame(round(B, 2), row.names = c(&quot;Topic.1&quot;, &quot;Topic.2&quot;))
names(B) &lt;- c(&quot;college&quot;, &quot;education&quot;, &quot;family&quot;, &quot;health&quot;, &quot;medicaid&quot;)</code></pre>
<p>Below is the table of the least squares estimate of<span class="math inline">\(B\)</span>:</p>
<pre class="r"><code>B</code></pre>
<pre><code>##         college education family health medicaid
## Topic.1    1.18      1.96  -0.02    0.6     0.58
## Topic.2    0.50      0.85   1.11    2.5     3.60</code></pre>
<p>Observe that row 1 of <span class="math inline">\(B\)</span> has high values in columns 1 and 2 compared to columns 3, 4, and 5; and row 2 has higher values for columns 4 and 5 compared to columns 1, 2, and 3. It is reasonable to infer that row 1 (<code>Topic.1</code>) refers to education, and row 2 (<code>Topic.2</code>) refers to health.</p>
<p>Below is the the table of the least squares estimate of <span class="math inline">\(Z\)</span>:</p>
<pre class="r"><code>Z</code></pre>
<pre><code>##            Topic.1 Topic.2
## document.1    3.13    0.05
## document.2   -1.55    3.58
## document.3    4.31    0.97
## document.4    0.41    2.71
## document.5   -1.16    2.68
## document.6    2.26    1.03</code></pre>
<p>Observe that <code>Topic.1</code> has big values in documents 1, 4, and 6. Likewise, <code>Topic.2</code> has big values in documents 2, 4, and 5. Hence, we can infer that documents 1, 4, and 6 are mostly about education; and documents 2, 4, and 5 are mostly about health.</p>
<p>We can use a scatterplot to explore the original five dimensional <span class="math inline">\(W\)</span> data in a two dimensional <span class="math inline">\(Z\)</span> data as follow:</p>
<pre class="r"><code>plot(x = Z$Topic.1, y = Z$Topic.2, cex = 3, 
     xlab = &quot;Topic.1&quot;, ylab = &quot;Topic.2&quot;)
text(x = Z$Topic.1, y = Z$Topic.2, labels= 1:6, cex= 1)</code></pre>
<p><img src="/post/2017-12-17-topic-modeling-the-intuition_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</section>
<section id="uniqueness-of-the-solution" class="level3">
<h3>Uniqueness of the solution</h3>
<p>The solution is not unique, as you might have noticed (note the difference in Z and B from the illustrative example and the computed Z and B) eventhough <span class="math inline">\(W\)</span> remains the same. To see why, assume <span class="math inline">\(T\)</span> is an orthonormal matrix, that is, <span class="math inline">\(T\)</span> is such that <span class="math inline">\(TT^t = I\)</span>. Then, <span class="math inline">\(W \simeq ZB = ZTT^tB = (ZT)(T^tB) = Z^*B^*\)</span> where <span class="math inline">\(Z^* = ZT\)</span>, and <span class="math inline">\(B^* = T^tB\)</span>. Thus, (<span class="math inline">\(Z\)</span>, <span class="math inline">\(B\)</span>) and (<span class="math inline">\(Z^*\)</span>, <span class="math inline">\(B^*\)</span>) are both equally valid solutions. Therefore, the solution is not unique. This non uniqueness of the solution poses some challenges for inferential studies based on the reduced dimension.</p>
</section>
<section id="interpreting-the-new-variables" class="level3">
<h3>Interpreting the new variables</h3>
<p>Again, we use the rows of the <span class="math inline">\(B\)</span> matrix to infer the meaning of each column of <span class="math inline">\(Z\)</span>. Why? Observed that <span class="math display">\[\hat B = (Z^tZ)^{-1}Z^tW\]</span> Let’s define <span class="math inline">\(F = (Z^tZ)^{-1}Z^t\)</span> with elements <span class="math inline">\(f_{i,j}\)</span>, that is, <span class="math inline">\(f_{i,j}\)</span> is the value in the <span class="math inline">\(i^{th}\)</span> row, <span class="math inline">\(j^{th}\)</span> column of the matrix <span class="math inline">\(F\)</span>. Thus, <span class="math inline">\(\hat B = FW\)</span> <span class="math display">\[
\hat B_{k,p}=\begin{bmatrix}b_{1,1} &amp; b_{1,2} &amp; \cdots &amp; b_{1,p}\\
b_{2,1} &amp; b_{2,2} &amp; \cdots &amp; b_{2,p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
b_{k,1} &amp; b_{k,2} &amp; \cdots &amp; b_{k,p}
\end{bmatrix}
=
\begin{bmatrix}f_{1,1} &amp; f_{1,2} &amp; \cdots &amp; f_{1,n}\\
f_{2,1} &amp; f_{2,2} &amp; \cdots &amp; f_{2,n}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
f_{k,1} &amp; f_{k,2} &amp; \cdots &amp; f_{k,n}
\end{bmatrix}
\begin{bmatrix}w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,p}\\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
w_{n,1} &amp; w_{n,2} &amp; \cdots &amp; w_{n,p}
\end{bmatrix}
\]</span></p>
<p>If you still remember matrix operations from high school, note that: <span class="math display">\[b_{1,1} = \sum_{l=1}^nf_{1,l}\times w_{l,1} \\
= f_{1,1}w_{1,1}+f_{1,2}w_{2,1}+f_{1,3}w_{3,1}+\cdots+f_{1,n}w_{n,1}\]</span> <span class="math display">\[b_{1,2} = \sum_{l=1}^nf_{1,l}\times w_{l,2} \\
= f_{1,1}w_{1,2}+f_{1,2}w_{2,2}+f_{1,3}w_{3,2}+\cdots+f_{1,n}w_{n,2}\]</span></p>
<p>Observe that the source of any numerical difference between <span class="math inline">\(b_{1,1}\)</span> and <span class="math inline">\(b_{1,2}\)</span> is the numerical difference between the first and second column of <span class="math inline">\(W\)</span> (the <span class="math inline">\(f_{i,j}\)</span> are exactly the same). Also, observe that, whatever <span class="math inline">\(F\)</span> is, <span class="math inline">\(b_{1,1}\)</span> is a total weight of the first variable <span class="math inline">\(W_1\)</span> (say the counts of word 1 in all the documents). Likewise, <span class="math inline">\(b_{1,2}\)</span> is a total weight of the second variable <span class="math inline">\(W_2\)</span> (say the count of the second word in all the documents); and so on untill <span class="math inline">\(b_{1,p}\)</span>. Put differently, <span class="math inline">\(b_{1,j}\)</span> is a total weight of the word <span class="math inline">\(W_j\)</span>. Thus, the coefficients <span class="math inline">\([b_{1,1},b_{1,2}, \cdots,b_{1,p}]\)</span> are the total weight of the words <span class="math inline">\(W_1, W_2, \cdots, W_p\)</span>, respectively. If these are words’ weights, it is natural to use the words with highest weights to name row 1 of <span class="math inline">\(B\)</span>. We name the remaining rows of <span class="math inline">\(B\)</span> in similar fashion.</p>
<p>Also, observe that the elements of the first row of <span class="math inline">\(B\)</span> are the coefficients of the first column of <span class="math inline">\(Z\)</span>. If row 1 of <span class="math inline">\(B\)</span> is named, say education for example, then the first column of <span class="math inline">\(Z\)</span> is an education variable. Hence, the naming of the columns of <span class="math inline">\(Z\)</span>.</p>
</section>
<section id="the-values-of-the-new-variables-z" class="level3">
<h3>The values of the new variables <span class="math inline">\(Z\)</span></h3>
<p>Again, we have <span class="math display">\[\hat Z = WB^t(BB^t)^{-1}\]</span> Let’s define <span class="math display">\[N = B^t(BB^t)^{-1}\]</span> Then <span class="math display">\[\hat Z = WN\]</span> That is <span class="math display">\[
\hat{Z} 
= 
\begin{bmatrix}
z_{1,1} &amp; z_{1,2} &amp; \cdots &amp; z_{1,k}\\
z_{2,1} &amp; z_{2,2} &amp; \cdots &amp; z_{2,k}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
z_{n,1} &amp; z_{n,2} &amp; \cdots &amp; z_{n,k}
\end{bmatrix} 
=
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,p}\\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
w_{n,1} &amp; w_{n,2} &amp; \cdots &amp; w_{n,p}
\end{bmatrix}
\begin{bmatrix}
n_{1,1} &amp; n_{1,2} &amp; \cdots &amp; n_{1,k}\\
n_{2,1} &amp; n_{2,2} &amp; \cdots &amp; n_{2,k}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
n_{p,1} &amp; n_{p,2} &amp; \cdots &amp; n_{p,k}
\end{bmatrix}
\]</span></p>
<p>Observe that <span class="math display">\[z_{1,1} = \sum_{m = 1}^p n_{m,1}w_{1,m} \\
 = n_{1,1}w_{1,1}+ n_{2,1}w_{1,2}+n_{3,1}w_{1,3}+\cdots+n_{p,1}w_{1,p}\]</span> <span class="math display">\[z_{1,2} = \sum_{m = 1}^p n_{m,2}w_{1,m} \\
= n_{1,2}w_{1,1}+ n_{2,2}w_{1,2}+n_{3,2}w_{1,3}+\cdots+n_{p,2}w_{1,p}\]</span> <span class="math display">\[z_{2,1} = \sum_{m = 1}^p n_{m,1}w_{2,m} \\
= n_{1,1}w_{2,1}+ n_{2,1}w_{2,2}+n_{3,1}w_{2,3}+\cdots+n_{p,1}w_{2,p}\]</span> The numerical difference between <span class="math inline">\(z_{1,1}\)</span> and <span class="math inline">\(z_{1,2}\)</span> stems from the numerical difference between the weights in column <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> of the weights matrix <span class="math inline">\(N\)</span> (<span class="math inline">\(N\)</span> can be seen as a weight matrix). The numerical difference between <span class="math inline">\(z_{1,1}\)</span> and <span class="math inline">\(z_{2,1}\)</span> stems from the numerical difference between the words counts in documents <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> of the words counts matrix <span class="math inline">\(W\)</span>.</p>
<p>Alternatively, we can think of <span class="math inline">\(Z\)</span> as a composite index matrix. <span class="math inline">\(z_{i,j}\)</span> is the value of the index <span class="math inline">\(j\)</span> in document <span class="math inline">\(i\)</span>. For example, <span class="math inline">\(z_{1,1}\)</span> is the value of index <span class="math inline">\(1\)</span> in document <span class="math inline">\(1\)</span>; <span class="math inline">\(z_{1,2}\)</span> is the value of index <span class="math inline">\(2\)</span> in document <span class="math inline">\(1\)</span>. Why different index values for the same document? Because each index assigns different weights to the same words. For index <span class="math inline">\(1\)</span>, the weights are the <span class="math inline">\(n_{m,1}\)</span> (<span class="math inline">\(m=\{1, 2,\cdots,p\}\)</span>). For the index <span class="math inline">\(2\)</span>, the weights are <span class="math inline">\(n_{m,2}\)</span>. And for the <span class="math inline">\(k^{th}\)</span> index, the weights are <span class="math inline">\(n_{m,k}\)</span>.</p>
</section>
</section>
</section>
<section id="some-variants-of-the-matrix-factorization" class="level1">
<h1>Some variants of the matrix factorization</h1>
<p>1- Note that our working example data <span class="math inline">\(W\)</span> is a count data. Naturally, we would want <span class="math inline">\(Z\)</span> and <span class="math inline">\(B\)</span> to have non-negative values. <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" target="_blank">Non-Negative Matrix Factorization</a> was invented to force the elements of <span class="math inline">\(Z\)</span>, and <span class="math inline">\(B\)</span> to be positive.</p>
<p>2- Moreover, the algorithm presented above assumes no probability distribution. Consequently, it is inapropriate to use <span class="math inline">\(Z\)</span> for inferential studies (Inferential studies build on probabilistic assumption of the data generating process). Probabilistic matrix factorization algorithms address these concerns. These methods include probabilistic Principal Component Analysis (PPCA), Multinomial Principal Component Analysis (mPCA), Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocation (LDA), etc…</p>
<p>3- Traditional matrix factorization methods implicitly or explicitly assume multivariate normal distribution, and decomposes the covariance matrix of the data. Factor Analysis (FA) and Principal Component Analysis (PCA) are two examples.</p>
<p>I hope this introductory exposition of topic modeling provides an intuitive understanding of the why, and how of the subject. Feel free to leave your comments below.</p>
</section>
